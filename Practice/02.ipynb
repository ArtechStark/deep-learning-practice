{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28 * 28\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./mnist_data/', \n",
    "                                          train=True, \n",
    "                                          transform=transforms.ToTensor(), \n",
    "                                          download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist_data/', \n",
    "                                         train=False, \n",
    "                                         transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n",
    "                                         batch_size = batch_size, \n",
    "                                         shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(input_size, num_classes)\n",
    "\n",
    "# Using CrossEntropyLoss, the label will be aotomatically converted to one-hot data.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5]\tBatch: [100\\600]\tLoss: 1.9844636917114258\n",
      "Epoch: [1/5]\tBatch: [200\\600]\tLoss: 2.003762722015381\n",
      "Epoch: [1/5]\tBatch: [300\\600]\tLoss: 1.9857181310653687\n",
      "Epoch: [1/5]\tBatch: [400\\600]\tLoss: 1.9906806945800781\n",
      "Epoch: [1/5]\tBatch: [500\\600]\tLoss: 1.967653751373291\n",
      "Epoch: [1/5]\tBatch: [600\\600]\tLoss: 1.9755593538284302\n",
      "Epoch: [2/5]\tBatch: [100\\600]\tLoss: 1.9709385633468628\n",
      "Epoch: [2/5]\tBatch: [200\\600]\tLoss: 1.9800925254821777\n",
      "Epoch: [2/5]\tBatch: [300\\600]\tLoss: 1.9560660123825073\n",
      "Epoch: [2/5]\tBatch: [400\\600]\tLoss: 1.9509189128875732\n",
      "Epoch: [2/5]\tBatch: [500\\600]\tLoss: 1.9749133586883545\n",
      "Epoch: [2/5]\tBatch: [600\\600]\tLoss: 1.9633018970489502\n",
      "Epoch: [3/5]\tBatch: [100\\600]\tLoss: 1.9403167963027954\n",
      "Epoch: [3/5]\tBatch: [200\\600]\tLoss: 1.9404596090316772\n",
      "Epoch: [3/5]\tBatch: [300\\600]\tLoss: 1.9458744525909424\n",
      "Epoch: [3/5]\tBatch: [400\\600]\tLoss: 1.9163146018981934\n",
      "Epoch: [3/5]\tBatch: [500\\600]\tLoss: 1.937270164489746\n",
      "Epoch: [3/5]\tBatch: [600\\600]\tLoss: 1.9292199611663818\n",
      "Epoch: [4/5]\tBatch: [100\\600]\tLoss: 1.9321987628936768\n",
      "Epoch: [4/5]\tBatch: [200\\600]\tLoss: 1.928457498550415\n",
      "Epoch: [4/5]\tBatch: [300\\600]\tLoss: 1.9258151054382324\n",
      "Epoch: [4/5]\tBatch: [400\\600]\tLoss: 1.9279729127883911\n",
      "Epoch: [4/5]\tBatch: [500\\600]\tLoss: 1.9181311130523682\n",
      "Epoch: [4/5]\tBatch: [600\\600]\tLoss: 1.922662615776062\n",
      "Epoch: [5/5]\tBatch: [100\\600]\tLoss: 1.9172548055648804\n",
      "Epoch: [5/5]\tBatch: [200\\600]\tLoss: 1.9257701635360718\n",
      "Epoch: [5/5]\tBatch: [300\\600]\tLoss: 1.9117705821990967\n",
      "Epoch: [5/5]\tBatch: [400\\600]\tLoss: 1.934240460395813\n",
      "Epoch: [5/5]\tBatch: [500\\600]\tLoss: 1.9318265914916992\n",
      "Epoch: [5/5]\tBatch: [600\\600]\tLoss: 1.9114500284194946\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        predicted = torch.sigmoid(model(images))\n",
    "        \n",
    "        loss = criterion(predicted, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch: [{}/{}]\\tBatch: [{}\\{}]\\tLoss: {}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8298\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = len(test_loader)*batch_size\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, dim=1)    # _和predicted分别代表了max()返回的10个数中最大值的具体值和索引，此处需要的只是索引\n",
    "        \n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Accuracy: {:.4f}'.format(correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'logistic_regression.ckpt')\n",
    "checkpoint = torch.load('logistic_regression.ckpt')\n",
    "model_ = nn.Linear(input_size, num_classes)\n",
    "model_.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.0023,  0.0246,  0.0085,  ..., -0.0120,  0.0308,  0.0317],\n",
       "                      [-0.0007, -0.0296, -0.0176,  ..., -0.0350, -0.0251, -0.0017],\n",
       "                      [ 0.0123, -0.0306,  0.0226,  ...,  0.0105,  0.0078,  0.0039],\n",
       "                      ...,\n",
       "                      [ 0.0338, -0.0147,  0.0056,  ..., -0.0021,  0.0070,  0.0329],\n",
       "                      [-0.0119,  0.0274, -0.0181,  ..., -0.0190,  0.0324,  0.0019],\n",
       "                      [-0.0084, -0.0049, -0.0236,  ..., -0.0333,  0.0138,  0.0224]])),\n",
       "             ('bias',\n",
       "              tensor([-0.0565,  0.0325, -0.0299,  0.0027, -0.0180, -0.0196,  0.0054,  0.0079,\n",
       "                      -0.0363, -0.0244]))])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

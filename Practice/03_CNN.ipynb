{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "hidden_size = 1000\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='mnist_data/', \n",
    "                                          train=True, \n",
    "                                          transform=transforms.ToTensor())\n",
    "test_dataset = torchvision.datasets.MNIST(root='mnist_data/', \n",
    "                                         train=False, \n",
    "                                         transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc2(self.relu(self.fc1(x)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5]\tStep: [100/600]\tLoss:0.4719\n",
      "Epoch: [1/5]\tStep: [200/600]\tLoss:0.1799\n",
      "Epoch: [1/5]\tStep: [300/600]\tLoss:0.1398\n",
      "Epoch: [1/5]\tStep: [400/600]\tLoss:0.1177\n",
      "Epoch: [1/5]\tStep: [500/600]\tLoss:0.2759\n",
      "Epoch: [1/5]\tStep: [600/600]\tLoss:0.1443\n",
      "Epoch: [2/5]\tStep: [100/600]\tLoss:0.0682\n",
      "Epoch: [2/5]\tStep: [200/600]\tLoss:0.1516\n",
      "Epoch: [2/5]\tStep: [300/600]\tLoss:0.1677\n",
      "Epoch: [2/5]\tStep: [400/600]\tLoss:0.0662\n",
      "Epoch: [2/5]\tStep: [500/600]\tLoss:0.0488\n",
      "Epoch: [2/5]\tStep: [600/600]\tLoss:0.0522\n",
      "Epoch: [3/5]\tStep: [100/600]\tLoss:0.1072\n",
      "Epoch: [3/5]\tStep: [200/600]\tLoss:0.0440\n",
      "Epoch: [3/5]\tStep: [300/600]\tLoss:0.0304\n",
      "Epoch: [3/5]\tStep: [400/600]\tLoss:0.0067\n",
      "Epoch: [3/5]\tStep: [500/600]\tLoss:0.0361\n",
      "Epoch: [3/5]\tStep: [600/600]\tLoss:0.0810\n",
      "Epoch: [4/5]\tStep: [100/600]\tLoss:0.0769\n",
      "Epoch: [4/5]\tStep: [200/600]\tLoss:0.0358\n",
      "Epoch: [4/5]\tStep: [300/600]\tLoss:0.0225\n",
      "Epoch: [4/5]\tStep: [400/600]\tLoss:0.0908\n",
      "Epoch: [4/5]\tStep: [500/600]\tLoss:0.0454\n",
      "Epoch: [4/5]\tStep: [600/600]\tLoss:0.0112\n",
      "Epoch: [5/5]\tStep: [100/600]\tLoss:0.0406\n",
      "Epoch: [5/5]\tStep: [200/600]\tLoss:0.0465\n",
      "Epoch: [5/5]\tStep: [300/600]\tLoss:0.0204\n",
      "Epoch: [5/5]\tStep: [400/600]\tLoss:0.0140\n",
      "Epoch: [5/5]\tStep: [500/600]\tLoss:0.0730\n",
      "Epoch: [5/5]\tStep: [600/600]\tLoss:0.0264\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch: [{}/{}]\\tStep: [{}/{}]\\tLoss:{:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9794\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = batch_size * len(test_loader)\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.detach(), 1)\n",
    "        \n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Accuracy: {:.4f}'.format((correct/total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'perception.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-0.0049, -0.0236, -0.0304,  ...,  0.0350, -0.0177,  0.0024],\n",
       "                      [-0.0081, -0.0236,  0.0084,  ..., -0.0350,  0.0133,  0.0306],\n",
       "                      [-0.0240, -0.0143, -0.0084,  ...,  0.0103, -0.0105, -0.0173],\n",
       "                      ...,\n",
       "                      [ 0.0318,  0.0232,  0.0097,  ...,  0.0293, -0.0032,  0.0230],\n",
       "                      [ 0.0265, -0.0015,  0.0119,  ...,  0.0226,  0.0018,  0.0006],\n",
       "                      [-0.0353,  0.0238,  0.0215,  ...,  0.0206, -0.0117,  0.0248]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.bias',\n",
       "              tensor([-4.4795e-02, -5.4820e-02,  2.9204e-02, -1.7473e-02,  1.0371e-02,\n",
       "                       6.7405e-02,  1.9405e-02, -1.4014e-02,  3.6928e-02,  6.3212e-03,\n",
       "                       1.7157e-02,  1.9340e-02,  2.8342e-02, -7.3563e-03,  8.9394e-03,\n",
       "                       6.1044e-03,  3.6285e-02,  2.4531e-02,  6.0869e-03, -4.0040e-02,\n",
       "                       1.4049e-02,  5.7094e-02,  8.1207e-02,  6.4823e-02,  8.1937e-02,\n",
       "                       7.0314e-03,  1.6823e-02, -7.0987e-03,  2.1720e-02,  4.8108e-02,\n",
       "                      -9.9302e-02,  5.1142e-03,  3.7763e-02, -7.3976e-03,  9.4420e-03,\n",
       "                      -4.6057e-02,  2.5475e-02,  4.8165e-02,  1.2425e-02, -7.3882e-03,\n",
       "                       1.2830e-02, -9.0512e-04, -2.4808e-02, -8.5790e-03, -3.6656e-02,\n",
       "                       1.2346e-02, -7.3626e-02,  2.9997e-02, -3.0791e-02, -7.0485e-02,\n",
       "                      -4.2240e-02,  1.5063e-02, -1.5647e-02, -3.8105e-02,  8.2121e-03,\n",
       "                      -4.3051e-03,  9.2767e-02,  7.0361e-02, -3.5309e-02, -9.4436e-03,\n",
       "                       3.7705e-02, -4.0443e-02,  7.1432e-02,  3.4963e-02,  7.2112e-02,\n",
       "                      -2.5219e-02,  3.7073e-02, -3.9732e-02, -4.6304e-03,  5.3753e-02,\n",
       "                       6.0862e-02, -2.9338e-02, -8.0072e-03, -5.2149e-02,  1.0091e-03,\n",
       "                       1.0113e-02,  1.8164e-02,  9.3177e-03,  4.2987e-02,  5.2388e-02,\n",
       "                       6.7606e-02,  4.6580e-02,  4.4053e-02, -4.0278e-04,  5.4576e-02,\n",
       "                       1.3355e-02, -5.9725e-02,  8.4652e-02,  6.2660e-02,  1.1443e-02,\n",
       "                      -1.3976e-02, -1.8676e-03,  1.7141e-02,  6.8037e-02, -4.0102e-03,\n",
       "                      -1.0445e-01,  2.5625e-02,  4.7781e-02,  5.3440e-03,  3.8965e-02,\n",
       "                      -2.1436e-02,  2.4306e-02, -4.4030e-03,  7.8767e-02, -1.0712e-02,\n",
       "                      -4.0753e-02, -5.3741e-02,  2.7714e-02, -3.4903e-02,  2.9529e-02,\n",
       "                      -4.5952e-02,  1.7627e-02, -2.4382e-02, -3.3164e-02,  1.8650e-02,\n",
       "                       4.0361e-02, -6.4431e-03, -2.1398e-02, -5.0201e-03, -4.1495e-02,\n",
       "                       3.2385e-02,  3.5939e-02, -2.4777e-02,  6.2441e-02, -3.6183e-02,\n",
       "                       1.3399e-02,  1.6637e-02,  1.4166e-01, -2.4613e-02,  2.8817e-02,\n",
       "                       5.1759e-02,  3.7390e-02,  2.9165e-02,  1.7424e-02, -4.4026e-02,\n",
       "                       4.7882e-02,  6.4691e-02,  4.1406e-02, -3.1379e-02,  1.5735e-02,\n",
       "                      -3.2327e-02,  4.2851e-02,  4.5573e-02,  9.5251e-02,  4.6027e-02,\n",
       "                       1.0963e-02,  2.7507e-02, -2.0128e-02,  1.0126e-01, -5.5764e-02,\n",
       "                      -2.2256e-02,  1.1391e-01,  9.8408e-03,  6.9245e-02, -1.6340e-02,\n",
       "                      -9.8881e-03,  8.7772e-03, -2.7512e-02,  4.3782e-02, -9.1031e-03,\n",
       "                      -8.4093e-03,  1.8185e-02,  5.6020e-02,  6.7777e-02, -6.4830e-02,\n",
       "                       2.0329e-02,  4.2869e-02,  3.7157e-03,  5.8820e-02,  1.6841e-02,\n",
       "                       2.5355e-02, -4.1890e-02,  2.8710e-02,  2.6694e-02,  1.5961e-01,\n",
       "                       4.5569e-02, -2.3681e-04, -3.8101e-02, -6.5520e-03,  6.7183e-03,\n",
       "                      -2.5979e-03,  2.5563e-02,  4.1237e-02, -2.8336e-03,  1.8406e-02,\n",
       "                       4.1327e-02, -1.5008e-02,  2.7678e-02, -3.1728e-02, -2.7209e-02,\n",
       "                       4.3598e-02,  3.7204e-02, -1.0204e-02,  1.6346e-02, -5.6862e-02,\n",
       "                      -2.3950e-02,  6.1181e-02, -2.1721e-02,  1.7123e-02,  1.8239e-02,\n",
       "                       7.5642e-02, -7.9257e-02,  4.7210e-02,  5.6382e-02,  2.8717e-02,\n",
       "                       3.2995e-03,  1.0468e-02, -3.7421e-02,  1.4704e-01,  5.0200e-02,\n",
       "                      -2.6894e-02, -2.3097e-02, -1.0365e-02,  2.4610e-03,  6.5729e-02,\n",
       "                       6.1104e-02,  4.5163e-02,  1.9653e-02,  6.3125e-02, -2.2650e-02,\n",
       "                       2.3776e-02,  1.9416e-02, -9.6215e-03,  8.8977e-02,  5.4999e-03,\n",
       "                      -3.1455e-02, -4.8031e-03, -1.3667e-02, -9.4162e-03,  4.8796e-02,\n",
       "                       1.0924e-01,  1.0484e-02,  3.9967e-02,  2.6921e-02, -2.4648e-02,\n",
       "                       1.0372e-01, -3.9999e-02,  4.6343e-04, -2.4910e-02,  2.6310e-02,\n",
       "                      -9.2656e-02, -1.0210e-02, -1.9235e-02, -1.7009e-02, -1.0143e-02,\n",
       "                       3.0291e-02, -3.2036e-02,  1.0395e-02,  3.5181e-02,  8.2426e-05,\n",
       "                       4.1958e-03, -2.6211e-02, -1.4361e-02, -2.9126e-02,  1.5370e-03,\n",
       "                       8.6548e-03,  1.0730e-01,  7.0647e-02,  5.2279e-02,  9.1027e-02,\n",
       "                      -5.8534e-02,  3.0493e-03, -1.5173e-03, -8.5069e-02, -2.7818e-03,\n",
       "                       4.8189e-02, -2.3257e-02,  8.8879e-03, -8.3249e-03,  5.1572e-02,\n",
       "                       3.2802e-02, -5.4487e-02,  3.0553e-03, -3.5300e-03,  2.1574e-02,\n",
       "                      -3.0494e-02,  6.4645e-02,  6.2099e-03, -5.3350e-02, -1.5374e-02,\n",
       "                      -1.2931e-04,  6.3036e-02, -3.3808e-02,  3.7272e-02, -3.7721e-02,\n",
       "                       2.5953e-03, -2.3771e-02,  1.5112e-02, -3.2273e-02,  4.5003e-02,\n",
       "                      -6.8813e-03,  2.4411e-02,  1.5179e-02,  7.3354e-02,  3.2956e-02,\n",
       "                       8.7877e-02, -2.6105e-03,  1.6248e-02, -7.0444e-02,  2.9385e-02,\n",
       "                      -2.7484e-02, -5.0695e-02,  1.7626e-02,  5.4279e-02, -1.4270e-02,\n",
       "                       8.6203e-02,  3.6015e-02, -2.4809e-02,  4.9138e-02, -3.5818e-03,\n",
       "                      -9.8218e-03, -1.4551e-03,  2.9548e-02,  3.1399e-02, -1.2109e-02,\n",
       "                      -1.0330e-02, -3.4179e-03,  1.0636e-02, -4.3054e-02,  7.7032e-02,\n",
       "                       3.7560e-02,  2.9824e-02, -8.5407e-03,  4.0322e-04,  4.7493e-02,\n",
       "                      -2.3816e-02, -3.8743e-02,  7.0091e-02,  3.0204e-02,  3.0699e-03,\n",
       "                      -4.2107e-02, -1.1722e-02, -1.2152e-03,  4.1218e-02, -4.9739e-02,\n",
       "                       2.6536e-02,  7.1847e-03, -5.1295e-02,  3.2396e-03,  3.0009e-02,\n",
       "                      -3.5745e-02,  7.2795e-03,  4.7258e-02, -2.0881e-02, -1.0388e-02,\n",
       "                       2.0898e-02,  6.6896e-02,  1.9298e-02, -9.6486e-03, -3.4238e-02,\n",
       "                       1.2765e-02,  5.6280e-03,  6.6605e-02,  1.9202e-03, -2.6915e-02,\n",
       "                      -3.6112e-03, -6.9999e-03,  4.4339e-02, -2.0597e-02,  3.6365e-02,\n",
       "                      -2.9368e-02, -3.5959e-02,  3.4733e-02, -4.0302e-03, -2.9297e-03,\n",
       "                      -2.7992e-02,  9.9294e-03,  6.5292e-02,  5.2579e-02, -6.8468e-03,\n",
       "                      -1.6290e-02, -2.7176e-03,  3.6439e-02,  1.3751e-02,  1.1285e-02,\n",
       "                      -1.6513e-02, -1.3851e-02,  8.5986e-02,  2.7640e-02,  1.8273e-03,\n",
       "                       3.2733e-02, -4.5105e-02,  2.8750e-03,  6.6737e-03,  3.5659e-02,\n",
       "                      -5.4600e-02,  1.5187e-03, -4.7678e-02,  1.3316e-02,  1.3756e-01,\n",
       "                      -1.5573e-02, -3.8129e-02,  7.9935e-02,  5.5471e-02, -2.7393e-02,\n",
       "                      -6.2296e-03,  3.2624e-02, -4.9552e-02, -7.9794e-02,  1.0005e-01,\n",
       "                       8.7267e-03, -6.8309e-03, -2.1417e-02,  1.2108e-02,  5.8704e-02,\n",
       "                      -7.1550e-03,  3.7470e-03,  4.6760e-02,  5.2400e-02,  7.6400e-02,\n",
       "                      -2.1691e-03, -1.0268e-02,  4.0833e-02, -2.1957e-02,  4.3331e-02,\n",
       "                       5.2695e-02,  3.6007e-02, -4.3088e-02,  2.2181e-02,  8.3240e-03,\n",
       "                       1.2430e-02, -2.9358e-02,  9.1318e-03, -1.7372e-02, -2.5705e-02,\n",
       "                      -1.9064e-02,  3.4292e-03,  9.6649e-02, -2.5487e-02, -5.6294e-02,\n",
       "                       3.5230e-03, -5.3951e-03, -3.7266e-02, -8.8515e-04,  1.3234e-02,\n",
       "                      -3.4810e-03,  3.5605e-02, -2.3128e-02,  2.3376e-02,  3.2327e-02,\n",
       "                       2.4707e-02,  1.5939e-02,  5.5471e-04, -4.0198e-02, -3.6915e-02,\n",
       "                      -7.4386e-04, -9.0713e-02,  7.7784e-03, -4.0747e-02,  1.6976e-02,\n",
       "                      -2.5296e-02, -1.5346e-02, -4.1751e-02, -5.7657e-03,  1.3377e-02,\n",
       "                      -3.2871e-02,  4.9501e-02, -5.4661e-02,  1.8065e-02,  6.1155e-02,\n",
       "                      -5.1546e-02, -2.3133e-02,  3.9620e-02,  7.9301e-03, -2.5107e-02,\n",
       "                       1.3259e-01, -2.6441e-02, -1.6968e-02, -2.0863e-02, -1.2933e-02,\n",
       "                       9.5533e-03,  7.4182e-02, -5.2603e-04,  6.3119e-02, -3.1200e-02,\n",
       "                      -2.9835e-02,  7.0703e-02,  3.0746e-02,  3.4100e-02, -4.8485e-03,\n",
       "                       3.7294e-02, -1.1935e-02, -7.5806e-02,  8.3653e-02,  4.5309e-02,\n",
       "                       4.0519e-02,  5.0572e-02, -2.5423e-02,  5.2043e-02, -7.2113e-03,\n",
       "                       4.5132e-02, -6.1121e-02,  2.6192e-02, -1.5754e-02, -5.2704e-02,\n",
       "                       4.4250e-02,  8.3366e-04,  5.9286e-02, -3.2843e-02,  1.2677e-02,\n",
       "                      -7.9730e-03, -3.9796e-02,  5.5733e-02,  5.6051e-02, -1.5769e-03,\n",
       "                       6.9654e-02, -1.7449e-02,  1.6194e-02,  3.9453e-02, -1.9941e-03,\n",
       "                       1.2733e-02, -6.4028e-03, -6.1750e-02,  8.1448e-02,  4.2990e-02,\n",
       "                       2.3256e-02,  2.7359e-02,  8.4500e-02, -2.6053e-02,  6.2395e-03,\n",
       "                       5.4587e-02,  8.6465e-02,  1.0416e-01,  2.7862e-02,  5.4665e-02,\n",
       "                      -3.9005e-02, -4.5951e-03, -1.3763e-02, -3.6512e-02, -1.4399e-02,\n",
       "                       4.1716e-02, -2.7251e-03, -3.4275e-02, -7.1504e-03,  3.7109e-02,\n",
       "                       2.5908e-02,  7.2716e-02,  3.4328e-02,  4.1511e-02, -1.3297e-02,\n",
       "                      -3.4354e-02, -4.5852e-02,  7.5118e-02,  4.6623e-03, -1.8412e-02,\n",
       "                      -4.8226e-02, -5.6598e-03, -4.2893e-02,  7.8708e-02, -1.3515e-02,\n",
       "                       1.0166e-01,  1.6804e-02,  1.1394e-02,  5.7592e-02,  3.6626e-02,\n",
       "                      -2.9336e-02,  3.0782e-02,  7.2347e-02,  1.4737e-02,  3.8507e-02,\n",
       "                       2.1033e-02, -4.2399e-02,  3.3176e-02, -3.1337e-03,  9.1526e-03,\n",
       "                       4.1303e-02, -4.1583e-02, -2.4376e-02,  2.4387e-02,  4.8298e-02,\n",
       "                      -4.2485e-02,  3.6238e-02,  1.4322e-02,  2.7841e-03, -2.1143e-02,\n",
       "                      -1.5344e-02, -1.0478e-02,  5.7754e-02,  1.5311e-02,  2.9815e-02,\n",
       "                      -2.8308e-02,  5.7246e-02, -3.3229e-03, -9.8788e-04,  1.1137e-02,\n",
       "                       5.4882e-02,  8.3487e-03, -1.6364e-02,  5.8162e-02, -1.1862e-03,\n",
       "                       2.9871e-02,  7.2616e-03,  2.3243e-02,  1.0503e-01,  2.7916e-02,\n",
       "                       1.0396e-01,  2.0604e-02, -2.7337e-02,  7.2587e-02, -5.6072e-02,\n",
       "                       4.7841e-02,  4.5874e-03,  1.2980e-01, -3.1271e-02, -2.3068e-03,\n",
       "                       6.9190e-03,  4.3598e-02, -2.7221e-02, -1.3977e-02,  4.9147e-02,\n",
       "                       9.6697e-03, -4.8235e-02,  6.4148e-02, -3.2090e-02,  8.5672e-04,\n",
       "                       6.9265e-02,  2.7535e-02, -1.1440e-02, -3.7184e-02, -3.1989e-02,\n",
       "                       1.1409e-02,  1.8846e-02, -5.9242e-02,  2.8686e-02,  1.1867e-02,\n",
       "                      -8.6597e-03,  7.8937e-02,  7.7706e-03,  1.0908e-01, -4.7840e-02,\n",
       "                       2.8986e-02,  6.8916e-02,  7.6439e-02, -1.2829e-02,  9.6337e-03,\n",
       "                       4.7273e-04, -2.0680e-02, -2.3569e-02,  2.4119e-02, -1.4669e-02,\n",
       "                       2.9960e-02,  9.0816e-04,  4.6692e-02, -5.4013e-03, -7.2836e-02,\n",
       "                      -1.4901e-02,  8.0390e-03, -8.0698e-02, -1.4167e-02, -6.7190e-02,\n",
       "                       7.3315e-02, -6.6358e-02,  8.2039e-02,  2.9336e-02,  1.9188e-02,\n",
       "                       3.8380e-02,  1.6714e-02,  1.2493e-01, -1.4628e-02, -5.3698e-03,\n",
       "                      -2.8685e-02,  1.2212e-02, -6.9885e-02, -2.1560e-02, -4.7660e-02,\n",
       "                       4.4594e-02,  5.8987e-02,  4.4262e-02, -6.0891e-02,  5.0719e-02,\n",
       "                      -3.0237e-02, -7.0321e-02,  1.0401e-01,  3.5761e-02, -1.3923e-02,\n",
       "                      -5.0002e-03,  3.1164e-02, -1.5753e-02,  1.4755e-02, -1.8656e-02,\n",
       "                      -2.0016e-02, -3.2555e-03, -3.2853e-02,  9.3375e-02,  4.2743e-02,\n",
       "                       3.5822e-02,  9.4567e-04,  2.6267e-02,  2.7473e-02,  1.1912e-02,\n",
       "                       2.1170e-02, -1.0275e-02,  2.7754e-02,  1.9407e-02, -3.4377e-02,\n",
       "                      -1.9684e-02,  1.9497e-02,  8.0004e-03,  4.4729e-02,  7.6981e-03,\n",
       "                       4.2160e-02,  8.9617e-03, -4.1416e-03, -2.9372e-02, -4.7037e-02,\n",
       "                      -6.2490e-02, -1.2964e-02,  9.6606e-02, -1.6816e-02, -1.3243e-02,\n",
       "                      -6.4868e-02,  2.2674e-02, -6.6482e-02,  1.0398e-02,  4.0196e-02,\n",
       "                      -3.1322e-02,  2.8283e-02,  1.5167e-02,  3.4160e-02, -4.8851e-02,\n",
       "                       7.3696e-02, -5.9857e-02,  3.2340e-02,  5.7232e-02,  3.3893e-02,\n",
       "                       3.6482e-02,  8.3661e-02,  9.1346e-03,  4.4478e-02, -1.7859e-02,\n",
       "                       2.2809e-02, -3.1589e-02, -2.1879e-03,  2.8372e-02,  2.7595e-02,\n",
       "                       1.2850e-01,  2.0411e-02,  5.4767e-02,  6.8791e-02,  2.2481e-02,\n",
       "                       2.1917e-02, -3.2569e-03, -5.5154e-02,  2.5459e-02,  7.6344e-02,\n",
       "                       5.0041e-02,  7.7764e-02,  2.3734e-02, -2.2856e-02, -7.8429e-02,\n",
       "                       2.0439e-02,  1.4988e-02,  5.5800e-02, -4.8525e-02,  1.6824e-02,\n",
       "                       3.7748e-02,  4.0200e-03, -1.4866e-02,  9.4933e-02, -2.1669e-02,\n",
       "                       3.0638e-03,  5.2902e-02,  2.3758e-02, -6.5715e-02, -2.5639e-02,\n",
       "                       3.2473e-02,  5.0198e-02,  7.9500e-02,  8.7284e-03,  7.6545e-02,\n",
       "                       5.5965e-02,  2.0477e-02, -1.1563e-02,  1.4983e-02,  8.9918e-05,\n",
       "                      -5.7314e-02,  7.8759e-03,  7.0034e-02,  9.7797e-02,  4.9427e-03,\n",
       "                      -3.0690e-02,  1.3631e-02, -2.0657e-02, -4.1537e-02,  7.4110e-02,\n",
       "                       2.3441e-02,  2.6665e-03,  6.8117e-02, -3.1302e-02,  1.2988e-02,\n",
       "                       3.6866e-02,  5.7920e-02,  3.1928e-02,  5.3276e-03, -7.4802e-02,\n",
       "                      -2.3389e-03,  2.4837e-02,  1.8931e-02,  6.1224e-02,  5.0033e-03,\n",
       "                       3.0637e-02,  1.1306e-01,  1.8303e-02,  3.7958e-02,  4.1485e-02,\n",
       "                       1.0213e-01,  2.3953e-02, -2.1874e-04, -2.3747e-02,  5.2086e-02,\n",
       "                      -7.0487e-02, -2.3847e-02,  6.9916e-02, -2.9846e-02, -4.8569e-02,\n",
       "                      -8.4387e-03,  6.7463e-04, -6.8843e-02,  6.5411e-02,  5.4809e-02,\n",
       "                       2.5724e-02,  5.5914e-02, -1.7495e-02,  1.4093e-02,  4.9521e-02,\n",
       "                       4.0936e-02, -7.0945e-03,  5.4717e-02, -2.5103e-03, -7.8809e-03,\n",
       "                      -3.8849e-02, -7.7191e-03, -2.8059e-02,  2.3981e-02,  7.8583e-02,\n",
       "                       1.7897e-02,  4.8181e-02,  1.3661e-02,  3.7661e-02, -1.4077e-02,\n",
       "                       5.0387e-02,  2.6068e-02,  1.6380e-02,  4.7302e-02,  3.4797e-02,\n",
       "                       1.1722e-02,  5.4820e-02,  4.0825e-02, -2.8573e-02,  3.0406e-02,\n",
       "                       3.1346e-02, -1.5999e-03,  7.3430e-02, -1.7510e-02,  2.0616e-02,\n",
       "                      -4.2316e-03, -8.8197e-03, -9.7384e-03, -4.6481e-03,  1.5105e-02,\n",
       "                       6.7462e-02,  7.3894e-03, -2.7118e-02, -3.3433e-02,  8.6677e-02,\n",
       "                      -1.5568e-02,  4.5742e-02,  3.3117e-03, -5.1024e-02,  1.9508e-02,\n",
       "                      -3.7247e-02,  5.5794e-02, -3.7389e-02, -3.0909e-02, -5.1153e-02,\n",
       "                       2.5676e-02,  4.4861e-02,  9.5942e-03,  4.3711e-02, -6.2051e-03,\n",
       "                       2.5412e-02,  1.2281e-02, -3.6149e-02,  7.2103e-02, -3.8994e-03,\n",
       "                       3.5978e-03,  8.3021e-02,  2.9437e-02, -6.7886e-02,  1.2790e-02,\n",
       "                      -4.5263e-02,  3.6754e-02,  2.7008e-02,  6.5138e-02,  4.9819e-02,\n",
       "                       1.0437e-01, -6.9770e-02, -2.3203e-02, -4.1630e-02,  4.1776e-03,\n",
       "                       7.2075e-02, -4.8395e-02,  4.3333e-02,  3.5216e-02, -5.2389e-04,\n",
       "                      -7.5608e-03, -1.3343e-02,  7.2852e-02,  2.0888e-02, -2.5479e-02,\n",
       "                       9.1575e-03,  2.4348e-02,  8.5252e-05, -1.3963e-02,  2.6647e-02,\n",
       "                       1.9701e-02, -2.3345e-03,  3.6172e-02, -1.8481e-02,  1.1196e-02,\n",
       "                       1.2417e-02,  6.6661e-02,  1.0453e-02,  5.3884e-02,  2.1590e-02,\n",
       "                      -4.8566e-02,  5.3132e-02, -3.8842e-02, -3.7490e-02,  6.4239e-02,\n",
       "                       3.8732e-02, -5.2598e-02, -1.1182e-01,  5.2573e-03, -6.4812e-02,\n",
       "                       8.5329e-03, -2.2894e-02,  1.8111e-02,  2.5895e-02,  4.5418e-02,\n",
       "                       2.5481e-02,  2.1933e-02,  4.6545e-02, -1.0435e-02, -8.5248e-03,\n",
       "                       1.2078e-01,  2.2343e-03,  7.5289e-02, -4.5220e-02,  5.2043e-02,\n",
       "                      -2.9620e-04, -8.3026e-03,  3.5909e-02, -7.1312e-02, -3.8638e-02,\n",
       "                      -5.4125e-02,  1.1316e-01,  4.6900e-02, -5.3009e-02,  1.6303e-02,\n",
       "                      -2.6036e-02, -2.2721e-02, -6.4773e-03,  9.8044e-02,  8.3578e-02,\n",
       "                       1.8377e-02,  1.8116e-02,  4.0174e-02, -8.8765e-02,  8.0094e-03,\n",
       "                       6.0095e-03, -3.9505e-03, -3.4086e-02,  2.8118e-02,  3.9174e-03,\n",
       "                       1.5980e-02, -3.8130e-02, -4.9769e-02,  2.7873e-02,  2.7995e-02,\n",
       "                       2.9828e-02,  1.6934e-03, -5.4852e-03,  1.5532e-02,  1.3394e-02,\n",
       "                       3.5348e-03, -4.1825e-02, -9.2474e-03,  3.1886e-02,  5.9624e-02,\n",
       "                       4.0935e-02, -5.8246e-04, -8.4658e-03,  4.7529e-02,  6.5059e-02,\n",
       "                       5.9753e-02, -4.9361e-02,  9.5733e-02,  8.2456e-03,  4.2877e-02],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.0950, -0.0227,  0.0579,  ...,  0.0616, -0.0101,  0.0226],\n",
       "                      [-0.0705, -0.0667, -0.0089,  ...,  0.2668,  0.0938, -0.0699],\n",
       "                      [-0.0364,  0.1218, -0.0956,  ...,  0.0439,  0.0053, -0.1004],\n",
       "                      ...,\n",
       "                      [-0.1368,  0.1831,  0.0159,  ...,  0.1374,  0.0031,  0.0366],\n",
       "                      [ 0.0233, -0.1476, -0.0292,  ..., -0.3039, -0.1646,  0.0336],\n",
       "                      [ 0.1755, -0.3117,  0.0144,  ..., -0.1197,  0.0514,  0.0143]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.0296, -0.0141, -0.0409, -0.0590,  0.0454,  0.0147, -0.0411, -0.0250,\n",
       "                       0.0514, -0.0032], device='cuda:0'))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
